import logging
from enum import Enum
import sys
import datetime

class LogIcons(Enum):
    INFO = "â„¹ï¸"
    SUCCESS = "âœ…"
    WARNING = "âš ï¸"
    ERROR = "âŒ"
    START = "ðŸš€"
    DATABASE = "ðŸ’¾"
    CODE = "ðŸ‘¨â€ðŸ’»"
    ANALYSIS = "ðŸ”"
    REPORT = "ðŸ“Š"

class Color:
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BLUE = '\033[94m'
    END = '\033[0m'

class CustomFormatter(logging.Formatter):
    """Custom log formatter with icons and colors"""
    
    def format(self, record):
        icon = getattr(LogIcons, record.levelname, LogIcons.INFO).value
        message = super().format(record)
        return f"{icon}  {message}"

def setup_logging():
    """Configure logging with visual formatting"""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(CustomFormatter('%(message)s'))
    logger.addHandler(handler)
    
    return logger

logger = setup_logging()

class ReportGenerator:
    """Generates reports with enhanced logging"""
    
    @staticmethod
    def generate_report(missing_indexes: Set[QueryCondition], 
                       output_path: str = "index-report",
                       formats: List[str] = ["text", "json", "html"]) -> None:
        """Generate reports with progress logging"""
        logger.info(f"{LogIcons.REPORT.value} Starting report generation")
        
        by_table = defaultdict(list)
        for condition in missing_indexes:
            by_table[condition.table_name].append(condition)
        
        for fmt in formats:
            try:
                if fmt == "text":
                    logger.info(f"Generating text report at {output_path}.txt")
                    ReportGenerator._generate_text_report(by_table, f"{output_path}.txt")
                elif fmt == "json":
                    logger.info(f"Generating JSON report at {output_path}.json")
                    ReportGenerator._generate_json_report(by_table, f"{output_path}.json")
                elif fmt == "html":
                    logger.info(f"Generating HTML report at {output_path}.html")
                    ReportGenerator._generate_html_report(by_table, f"{output_path}.html")
                
                logger.info(f"{LogIcons.SUCCESS.value} {fmt.upper()} report generated successfully")
            except Exception as e:
                logger.error(f"Failed to generate {fmt} report: {str(e)}")
        
        logger.info(f"{LogIcons.SUCCESS.value} Report generation completed")

class IndexAnalyzer:
    """Enhanced with detailed logging"""
    
    def run_analysis(self, repo_path: str, flyway_path: str) -> None:
        logger.info(f"{LogIcons.START.value} Starting database index analysis")
        
        logger.info(f"{LogIcons.CODE.value} Scanning repository at {repo_path}")
        self._load_entity_metadata(repo_path)
        
        logger.info(f"{LogIcons.DATABASE.value} Checking Flyway migrations at {flyway_path}")
        self._load_existing_indexes(flyway_path)
        
        logger.info(f"{LogIcons.ANALYSIS.value} Analyzing query patterns")
        self._analyze_and_report()
        
        logger.info(f"{LogIcons.SUCCESS.value} Analysis completed")

    def _load_entity_metadata(self, repo_path: str) -> None:
        logger.info("Extracting entity metadata from Kotlin files...")
        file_count = 0
        entity_count = 0
        
        for kotlin_file in Path(repo_path).rglob('*.kt'):
            file_count += 1
            try:
                with open(kotlin_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if metadata := self.metadata_extractor.extract_entity_metadata(content, str(kotlin_file)):
                        self.entity_metadata[metadata.table_name] = metadata
                        entity_count += 1
                        logger.debug(f"Found entity: {metadata.table_name} in {kotlin_file}")
            except Exception as e:
                logger.warning(f"Error processing {kotlin_file}: {str(e)}")
        
        logger.info(f"{LogIcons.SUCCESS.value} Processed {file_count} files, found {entity_count} entities")

    def _load_existing_indexes(self, flyway_path: str) -> None:
        logger.info("Checking existing indexes in Flyway migrations...")
        migration_count = 0
        index_count = 0
        
        for migration in Path(flyway_path).glob('*.sql'):
            migration_count += 1
            try:
                with open(migration, 'r', encoding='utf-8') as f:
                    content = f.read()
                    indexes_before = sum(len(v) for v in self.existing_indexes.values())
                    self._parse_migration(content)
                    indexes_added = sum(len(v) for v in self.existing_indexes.values()) - indexes_before
                    index_count += indexes_added
                    
                    if indexes_added > 0:
                        logger.debug(f"Found {indexes_added} indexes in {migration}")
            except Exception as e:
                logger.warning(f"Error processing {migration}: {str(e)}")
        
        logger.info(f"{LogIcons.SUCCESS.value} Processed {migration_count} migrations, found {index_count} indexes")

    def _analyze_and_report(self) -> None:
        query_conditions = self.query_analyzer.analyze_repositories(str(Path.cwd()))
        missing_indexes = self._identify_missing_indexes(query_conditions)
        
        if missing_indexes:
            logger.warning(f"{LogIcons.WARNING.value} Found {len(missing_indexes)} potentially missing indexes")
        else:
            logger.info(f"{LogIcons.SUCCESS.value} No missing indexes detected")
        
        self._generate_report(missing_indexes)

def main():
    try:
        logger.info(f"{LogIcons.START.value} Starting Database Index Analyzer")
        logger.info(f"{Color.BLUE}Version: 1.0.0 | Python 3.6 Compatible{Color.END}")
        
        parser = argparse.ArgumentParser(
            description="Enterprise Database Index Analyzer with Visual Logging"
        )
        # ... (existing argument parsing code)
        
        args = parser.parse_args()
        
        logger.info("Configuration:")
        logger.info(f"  Repository path: {args.repo_path}")
        logger.info(f"  Flyway path: {args.flyway_path}")
        logger.info(f"  Fail threshold: {args.fail_threshold}")
        logger.info(f"  Output formats: {', '.join(args.formats)}")
        
        # Dependency injection
        metadata_extractor = KotlinEntityExtractor()
        query_analyzer = SpringDataQueryAnalyzer(metadata_extractor)
        analyzer = PipelineIndexAnalyzer(metadata_extractor, query_analyzer)
        
        analyzer.set_fail_threshold(args.fail_threshold)
        success = analyzer.run_pipeline_analysis(args.repo_path, args.flyway_path)
        
        # Generate reports
        query_conditions = query_analyzer.analyze_repositories(args.repo_path)
        missing_indexes = analyzer._identify_missing_indexes(query_conditions)
        
        logger.info(f"{LogIcons.REPORT.value} Generating reports...")
        ReportGenerator.generate_report(
            missing_indexes,
            output_path=args.output_prefix,
            formats=args.formats
        )
        
        if success:
            logger.info(f"{LogIcons.SUCCESS.value} Analysis completed successfully")
        else:
            logger.error(f"{LogIcons.ERROR.value} Analysis failed due to missing indexes")
        
        sys.exit(0 if success else 1)
        
    except Exception as e:
        logger.error(f"{LogIcons.ERROR.value} Fatal error: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
